{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Простая классификация по тональности\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем данные с сайта Кинопоиск: последние 200 положительный, 200 нейтральных и 200 отрицательных отзывов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 200 200\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import urllib2\n",
    "import urllib\n",
    "from lxml import html\n",
    "from newspaper import Article\n",
    "bad_main_url = 'http://www.kinopoisk.ru/reviews/type/comment/status/bad/'\n",
    "bad_urls = []\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_2) AppleWebKit/600.3.18 (KHTML, like Gecko) Version/8.0.3 Safari/600.3.18'\n",
    "values = {'name' : 'HSE', 'location' : 'Russia', 'language' : 'Nohtyp' }\n",
    "headers = { 'User-Agent' : user_agent }\n",
    "data = urllib.urlencode(values)\n",
    "\n",
    "def get_review_urls(main_url):\n",
    "    urls = []\n",
    "    for i in range(1, 21):\n",
    "        cur_url = main_url + 'period/month/page/'+str(i)+'/#list'\n",
    "        req = urllib2.Request(cur_url, data, headers)\n",
    "        response = urllib2.urlopen(req)\n",
    "        page = response.read()\n",
    "        code = ''\n",
    "        for line in page:\n",
    "            code += line\n",
    "        tree = html.fromstring(code)\n",
    "        all_urls = tree.xpath('//a/@href')\n",
    "        for url in all_urls:\n",
    "            if 'user' in url and 'comment' in url and '#comm0' not in url:\n",
    "                if not url in bad_urls:\n",
    "                    urls.append(url)\n",
    "    return urls\n",
    "\n",
    "bad_main_url = 'http://www.kinopoisk.ru/reviews/type/comment/status/bad/'\n",
    "bad_urls = get_review_urls(bad_main_url)\n",
    "\n",
    "good_main_url = 'http://www.kinopoisk.ru/reviews/type/comment/status/good/'\n",
    "good_urls = get_review_urls(good_main_url)\n",
    "\n",
    "neutral_main_url = 'http://www.kinopoisk.ru/reviews/type/comment/status/neutral/'\n",
    "neutral_urls = get_review_urls(neutral_main_url)\n",
    "\n",
    "print len(bad_urls), len(good_urls), len(neutral_urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсер html страничек с отзывавами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from HTMLParser import HTMLParser\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.data = []\n",
    "    def handle_data(self, data):\n",
    "        self.data.append(data)\n",
    "\n",
    "\n",
    "def getData(text, pattern):\n",
    "    p = re.compile(pattern, re.DOTALL)\n",
    "    try:\n",
    "        m = p.search(text).group()\n",
    "    except AttributeError:\n",
    "        return\n",
    "    else:\n",
    "        parser = MyHTMLParser()\n",
    "        parser.feed(m)\n",
    "        return parser.data\n",
    "\n",
    "def get_reviews(urls):\n",
    "    texts = []\n",
    "    for url in urls:\n",
    "        comment_url = 'http://www.kinopoisk.ru'+ url\n",
    "        req = urllib2.Request(comment_url, data, headers)\n",
    "        response = urllib2.urlopen(req)\n",
    "        page = response.read()\n",
    "        code = ''\n",
    "        text_p = '<span class=\"_reachbanner_\" itemprop=\"reviewBody\">.*?</span>'\n",
    "        for line in page:\n",
    "            code += line\n",
    "        text = getData(code, text_p)\n",
    "        texts.append(' '.join(text).decode('windows-1251'))\n",
    "    return texts\n",
    "\n",
    "bad_reviews = get_reviews(bad_urls)\n",
    "good_reviews = get_reviews(good_urls)\n",
    "neutral_reviews = get_reviews(neutral_urls)\n",
    "\n",
    "print len(bad_reviews)\n",
    "print len(good_reviews)\n",
    "print len(neutral_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['reviews'] = []\n",
    "for i in bad_reviews: \n",
    "    data['reviews'].append({'review':i, 'class':'negative'})\n",
    "for i in good_reviews: \n",
    "    data['reviews'].append({'review':i, 'class':'positive'})\n",
    "for i in neutral_reviews: \n",
    "    data['reviews'].append({'review':i, 'class':'neutral'})\n",
    "\n",
    "import json\n",
    "with open('data/kino_data.txt', 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартная предобработка данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "exclude = set(punctuation + u'0123456789[]—«»')\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "def preprocess(text):\n",
    "    buf = ''.join(ch for ch in text if ch not in exclude)\n",
    "    tokens = WhitespaceTokenizer().tokenize(buf.lower())\n",
    "    lemmatizer = MorphAnalyzer()\n",
    "    lemmas = []\n",
    "    for t in tokens[:]:\n",
    "        if not t in stopwords.words('russian'):\n",
    "            try:\n",
    "                lemma = lemmatizer.parse(t)[0].normal_form\n",
    "            except: \n",
    "                lemma = t\n",
    "#             print t, type(t), lemma\n",
    "            lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск предобработки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of reviews: 600\n",
      "случай весь текст нужный правильно расставлять кавычка понять свой ум автор первый наверное самый важный премьер год фильм который пропустить кой случай триумфальный возвращение алекс пройас большой кино очередной подтверждение безупречный реноме джерард батлера серьёзный придирчивый относиться выбор роль актёр долгожданный пропуск николай костеравальдау общество актёр класс девочка прошлогодний безумный макс нужный счастие перейти поразительный потрясать плюс картина взгляд минус вообще такой произведение задаваться вопрос выйти долгожданный продолжение фильм битва титан гнев титан бог египет весь свой красота давать положительный ответ кульминационный пиршество время битва бог титан монстр коготь заключить именно творение алекс пройас прежде вызволить любимый гнёт один важный египетский шишка молодой юнец требоваться замедлить кадр обязательно поцеловать девушка нужный убегать нога вперёд поцелуй важный древний египет мочь найтись юноша тело райана рейнольдс дева тело моника беллучча абсолютно детский невинный личико символизировать чистый любовь это порядочный девушка нужный позволять воришка вешать вермишель ухо действительно втирать какуюто дичь большой дом дюжина рот читать ребёнок беззаботный будущий бек задумать достигнуть благополучие путём присвоение дорогой вещий заработать человек непосильный труд который американский кинодеятель объяснять зритель бывать сильный бог слабый бог сет лишь сын великое р остальной божество спартанский рык мгновенно превращаться покорный овечка помощь тайна правительственный программа супербог пуститься тяжкий начать беседовать каждый бог отдельность древний египет настолько продвинуть цивилизация запустить собственный космический станция капитан бортпроводник старший офицер механик который являться всемогущий р днями ночь пускать червятрансформер огонёк сожрать мирный цивилизация поглотить гигантский паразит всё вселенная поддаваться древний египет божествопроводник умерший загробный миро анубис настолько трудоголик следить весь новое душа подземелье лично трудоголик легко сорвать рабочий будни парить мозги повод тот влюбить детский лик бог носиться этот смертный бог египет это также история дим билан когдато право бог уверенный чтоть невозможный конец всё стать возможный просто добавить вод поверьте бог египет полнымполно положительный момент который захватить дух фильм смотреть удовольствие весь семья остаться восторг дедушка который знать история мифология пожалуйста воспринимать текст описание искренний эмоция забывать нужный знак препинание это самый случай сложный подобрать оценка оценка какой хотеть поставить природа увы существовать\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('data/kino_data.txt') as infile:\n",
    "    data = json.load(infile)\n",
    "print 'N of reviews:', len(data['reviews'])\n",
    "texts = []\n",
    "target = []\n",
    "for r in data['reviews']:\n",
    "    text = ' '.join(preprocess(r['review'])).strip()\n",
    "    texts.append(text)\n",
    "    target.append(r['class'])\n",
    "print texts[0]\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150 случайных документов – обучающая выборка, остальные 50 - тестовая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[148, 62, 90, 13, 161, 188, 171, 186, 163, 118, 79, 122, 130, 150, 27, 87, 71, 119, 59, 123, 172, 101, 193, 134, 30, 22, 113, 177, 29, 78, 105, 120, 129, 89, 55, 195, 175, 81, 65, 9, 151, 165, 94, 164, 200, 99, 25, 115, 40, 141, 179, 70, 108, 104, 167, 91, 52, 8, 124, 176, 73, 166, 103, 33, 43, 153, 69, 180, 17, 147, 86, 97, 54, 85, 18, 125, 154, 49, 191, 23, 12, 199, 20, 135, 156, 42, 57, 128, 77, 48, 136, 36, 11, 183, 149, 24, 162, 67, 132, 31, 32, 190, 98, 182, 144, 35, 46, 5, 15, 68, 142, 83, 95, 185, 10, 197, 181, 155, 110, 189, 187, 143, 4, 168, 146, 158, 16, 174, 14, 126, 66, 137, 184, 88, 37, 39, 58, 74, 169, 44, 21, 1, 139, 60, 138, 117, 152, 198, 159, 76, 226, 228, 378, 298, 324, 271, 349, 213, 306, 283, 366, 217, 216, 206, 372, 249, 255, 267, 278, 269, 319, 258, 316, 337, 307, 367, 290, 246, 382, 265, 294, 305, 397, 286, 398, 385, 275, 351, 288, 387, 357, 386, 214, 338, 327, 329, 325, 368, 320, 289, 256, 391, 395, 360, 250, 225, 312, 364, 393, 321, 389, 394, 210, 245, 317, 373, 308, 304, 303, 291, 342, 300, 231, 359, 232, 344, 333, 323, 301, 221, 230, 328, 335, 273, 284, 243, 353, 266, 369, 235, 400, 377, 354, 203, 296, 270, 343, 313, 219, 384, 302, 309, 348, 208, 350, 371, 336, 274, 224, 262, 251, 201, 285, 282, 299, 339, 247, 347, 218, 390, 362, 236, 260, 345, 341, 237, 295, 234, 220, 264, 233, 279, 229, 211, 209, 376, 292, 375, 331, 223, 322, 293, 392, 346, 212, 280, 287, 257, 326, 297, 453, 439, 559, 501, 564, 569, 549, 434, 459, 507, 578, 467, 548, 431, 436, 502, 528, 401, 582, 587, 468, 454, 555, 566, 402, 494, 539, 414, 493, 552, 553, 550, 530, 557, 474, 544, 596, 426, 417, 521, 505, 514, 405, 412, 537, 428, 400, 497, 534, 565, 562, 574, 533, 503, 490, 408, 568, 463, 470, 437, 529, 586, 411, 541, 461, 519, 458, 576, 445, 551, 485, 481, 413, 440, 531, 496, 591, 513, 594, 590, 498, 527, 420, 477, 589, 554, 433, 535, 471, 448, 427, 536, 593, 511, 418, 438, 430, 472, 451, 483, 532, 579, 447, 499, 504, 577, 575, 446, 444, 516, 480, 404, 449, 455, 599, 580, 558, 567, 409, 432, 510, 406, 560, 543, 570, 584, 407, 581, 563, 429, 546, 456, 515, 419, 465, 512, 486, 482, 520, 561, 542, 506, 460, 492, 592, 488, 489, 540, 571, 598]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_idx = random.sample(range(0,201),  150) + random.sample(range(200, 401),  150) + random.sample(range(400, 600),  150) \n",
    "print train_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формирование обучающей выборки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "train_texts = []\n",
    "train_target = []\n",
    "test_texts = []\n",
    "test_target = []\n",
    "\n",
    "for t_id in range(len(texts)):\n",
    "    if t_id in train_idx:\n",
    "        train_texts.append(texts[t_id])\n",
    "        train_target.append(target[t_id])\n",
    "    else:\n",
    "        test_texts.append(texts[t_id])\n",
    "        test_target.append(target[t_id])        \n",
    "        \n",
    "\n",
    "print train_target.count('neutral') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация на три класса и матрица ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34 11  6]\n",
      " [15 20 15]\n",
      " [ 9 12 29]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer()),\n",
    "    ('classifier',  MultinomialNB()) ])\n",
    "\n",
    "pipeline.fit(train_texts, train_target)\n",
    "predicted = pipeline.predict(test_texts)\n",
    "\n",
    "print metrics.confusion_matrix(test_target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация на три класса и матрица ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[79 21]\n",
      " [31 70]]\n"
     ]
    }
   ],
   "source": [
    "train_idx = random.sample(range(0,201),  100) + random.sample(range(200, 401),  100)# + random.sample(range(400, 600),  150) \n",
    "train_texts = []\n",
    "train_target = []\n",
    "test_texts = []\n",
    "test_target = []\n",
    "\n",
    "for t_id in range(400):\n",
    "    if t_id in train_idx:\n",
    "        train_texts.append(texts[t_id])\n",
    "        train_target.append(target[t_id])\n",
    "    else:\n",
    "        test_texts.append(texts[t_id])\n",
    "        test_target.append(target[t_id])  \n",
    "        \n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer()),\n",
    "    ('classifier',  MultinomialNB()) ])\n",
    "\n",
    "pipeline.fit(train_texts, train_target)\n",
    "predicted = pipeline.predict(test_texts)\n",
    "\n",
    "print metrics.confusion_matrix(test_target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Классификация на два класса и матрица ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (SageMath)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "Russian_kinopoisk_NB_classification.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
