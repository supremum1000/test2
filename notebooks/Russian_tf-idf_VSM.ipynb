{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мера контрастности tf-idf для выделения ключевых слов. Векторная модель текста. Снижение размерности в векторной модели. Совсем простой поиск\n",
    "\n",
    "\n",
    "## Мера контрастности $tf-idf$ для выделения ключевых слов\n",
    "\n",
    "Рассмотрим коллекцию текстов о ягуарах. Ягуар – это омоним, имеет несколько значений: животное, название напитка, название автомобильной марки, название фильма и модель швейной машины. На каждое из 5 значений слова у нас есть по 5 текстов. Для каждого из 5 значений слова мы можем найти в коллекции свои, специфические слова. \n",
    "\n",
    "Для начала объединим каждые 5 текстов в один текст, получим коллекцию из 5 текстов, каждый посвящен своему значению. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ktr/NLP/data/jaguar1/1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3f547b049c8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcur_collection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mcur_collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmerged_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_collection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/ktr/NLP/data/jaguar1/1.txt'"
     ]
    }
   ],
   "source": [
    "path = '/Users/ktr/NLP/data/jaguar'\n",
    "raw_collection = []\n",
    "for i in range(1,6):\n",
    "    cur_path = path + str(i) + '/'\n",
    "    cur_collection = []\n",
    "    for j in range(1,6):\n",
    "        lines = open(cur_path+str(j)+'.txt').readlines()\n",
    "        cur_collection.append(lines[0].decode('utf-8'))\n",
    "    merged_text = ' '.join(cur_collection)\n",
    "    print merged_text[:100]\n",
    "    raw_collection.append(merged_text)\n",
    "print len(raw_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Проведем стандартную предобработку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "exclude = set(punctuation + u'0123456789[]—«»–')\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "def preprocess(text):\n",
    "    buf = ''.join(ch for ch in text if ch not in exclude)\n",
    "    tokens = WhitespaceTokenizer().tokenize(buf.lower())\n",
    "    lemmatizer = MorphAnalyzer()\n",
    "    lemmas = []\n",
    "    for t in tokens[:]:\n",
    "        if not t in stopwords.words('russian'):\n",
    "            try:\n",
    "                lemma = lemmatizer.parse(t)[0].normal_form\n",
    "            except: \n",
    "                lemma = t\n",
    "#             print t, type(t), lemma\n",
    "            lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "# lemmatizer = MorphAnalyzer()    \n",
    "# print lemmatizer.parse(u'фильмы')[0].normal_form "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем реализации $tf-idf$ из NLTK для поиска ключевых слов в кажлом тексте. Существенный недостаток этой реализации – скорость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3fe11fe6f411>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextCollection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcur_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollection\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcur_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "collection = [preprocess(text) for text in raw_collection]\n",
    "from nltk.text import TextCollection\n",
    "from nltk.corpus import stopwords \n",
    "cur_text = 1\n",
    "buf = []\n",
    "corpus = TextCollection(collection)\n",
    "for i in set(collection[cur_text]):\n",
    "    buf.append([ i, corpus.tf_idf(i, collection[cur_text])])\n",
    "for i in sorted(buf,key=lambda l:l[1], reverse=True)[:20]:\n",
    "    print i[0], i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторная модель текста\n",
    "\n",
    "Вернемся к исходным 25 текстам и снова проведем стандартную обработку. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ktr/NLP/data/jaguar1/1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1679c731d1b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcur_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mraw_collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_collection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/ktr/NLP/data/jaguar1/1.txt'"
     ]
    }
   ],
   "source": [
    "path = '/Users/ktr/NLP/data/jaguar'\n",
    "raw_collection = []\n",
    "for i in range(1,6):\n",
    "    cur_path = path + str(i) + '/'\n",
    "    for j in range(1,6):\n",
    "        lines = open(cur_path+str(j)+'.txt').readlines()\n",
    "        raw_collection.append(lines[0].decode('utf-8'))\n",
    "print len(raw_collection)\n",
    "collection = [preprocess(text) for text in raw_collection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вычислим косинусную меру сходства между всеми текстами с помощью библиотеки gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot index a corpus with zero features (you must specify either `num_features` or a non-empty corpus in the constructor)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-65b5bc6b8daa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcorpus_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMatrixSimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0msims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus_tfidf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/projects/b2281ee7-8388-4f84-a6b1-3125dd218512/.local/lib/python2.7/site-packages/gensim/similarities/docsim.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_best, dtype, num_features, chunksize, corpus_len)\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_features\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot index a corpus with zero features (you must specify either `num_features` or a non-empty corpus in the constructor)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"creating matrix with %i documents and %i features\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot index a corpus with zero features (you must specify either `num_features` or a non-empty corpus in the constructor)"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import numpy as np\n",
    "\n",
    "dictionary = corpora.Dictionary(collection)\n",
    "corpus = [dictionary.doc2bow(t) for t in collection]\n",
    "tfidf = models.TfidfModel(corpus) \n",
    "corpus_tfidf = tfidf[corpus]\n",
    "index = similarities.MatrixSimilarity(tfidf[corpus])\n",
    "sims = index[corpus_tfidf]\n",
    "print np.around(sims, decimals = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как сама по себе матрица не очень понятна, построим по ней диаграмму близости. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sims' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6f7c8380ac1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpcolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mcolorbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sims' is not defined"
     ]
    }
   ],
   "source": [
    "from pylab import pcolor, show, colorbar, xticks, yticks\n",
    "%matplotlib inline\n",
    "\n",
    "pcolor(sims)\n",
    "colorbar()\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь используем ЛСА, снизим количество размерностей до 10, пересчитаем и перерисуем матрицу близости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "decomposition not initialized yet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-496aed816530>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlsi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcorpus_lsi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlsi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMatrixSimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlsi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus_lsi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpcolor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/projects/b2281ee7-8388-4f84-a6b1-3125dd218512/.local/lib/python2.7/site-packages/gensim/models/lsimodel.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, bow, scaled, chunksize)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m         \"\"\"\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mu\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"decomposition not initialized yet\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;31m# if the input vector is in fact a corpus, return a transformed corpus as a result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: decomposition not initialized yet"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus, num_topics=10) \n",
    "corpus_lsi = lsi[corpus]\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "sims = index[corpus_lsi]\n",
    "pcolor(sims)\n",
    "colorbar()\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "Создайте собственную коллекцию текстов. Найдите ключевые слова для каждого текста по мере $tf-idf$. Объясните принцип создания коллекции и полученные результаты. \n",
    "\n",
    "# Задание 2\n",
    "\n",
    "Найдите ключевые биграммы для каждого текста по мере $tf-idf$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Литература\n",
    "\n",
    "* Manning & Raghavan & Schütze (IRbook) – Ch. 6.2.2 – Tf-idf weighting (http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html)\n",
    "\n",
    "* Manning & Raghavan & Schütze (IRbook) – Ch. 18 – Matrix decompositions and latent semantic indexing\n",
    "\n",
    "* NLTK (плохая документация):  http://www.nltk.org/api/nltk.html#nltk.text.TextCollection\n",
    "\n",
    "* http://www.tfidf.com/\n",
    "\n",
    "* Gensim (документация и туториал): https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Совсем простой поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 0.3106142)\n",
      "(20, 0.082819626)\n",
      "(14, 0.063121535)\n",
      "(24, 0.060640387)\n",
      "(21, 0.055787817)\n",
      "(12, 0.026171768)\n",
      "(23, 0.024431361)\n",
      "(11, 0.020480255)\n",
      "(10, 0.018185819)\n",
      "(13, 0.01243538)\n"
     ]
    }
   ],
   "source": [
    "doc = u\"машина\"\n",
    "vec_bow = dictionary.doc2bow(preprocess(doc))\n",
    "vec_tfidf = tfidf[vec_bow] \n",
    "sims = index[vec_tfidf]\n",
    "sim_list = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for i in sim_list:\n",
    "    if i[1] > 0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 0.048979394)\n",
      "(21, 0.048894458)\n",
      "(0, 0.026018053)\n",
      "(2, 0.024824783)\n",
      "(8, 0.024074724)\n",
      "(13, 0.023272306)\n",
      "(18, 0.021200787)\n",
      "(19, 0.0207118)\n",
      "(15, 0.018684626)\n",
      "(24, 0.012875895)\n",
      "(1, 0.012719706)\n",
      "(23, 0.010706264)\n",
      "(6, 0.010033874)\n",
      "(4, 0.0097972434)\n",
      "(3, 0.0085014161)\n",
      "(9, 0.0082245413)\n",
      "(14, 0.0037426921)\n",
      "(10, 0.0028754654)\n",
      "(20, 0.0026599406)\n",
      "(5, 0.0025733248)\n",
      "(17, 0.0024919596)\n",
      "(16, 0.0017642245)\n",
      "(11, 0.0016191259)\n"
     ]
    }
   ],
   "source": [
    "doc = u\"хороший ягуар\"\n",
    "vec_bow = dictionary.doc2bow(preprocess(doc))\n",
    "vec_tfidf = tfidf[vec_bow] \n",
    "sims = index[vec_tfidf]\n",
    "sim_list = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for i in sim_list:\n",
    "    if i[1] > 0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тематические модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.614*\"ягуар\" + 0.216*\"год\" + 0.146*\"нитка\" + 0.141*\"напиток\" + 0.137*\"jaguar\" + 0.132*\"который\" + 0.129*\"свой\" + 0.121*\"человек\" + 0.120*\"швейный\" + 0.112*\"машинка\"\n",
      "***\n",
      "-0.341*\"нитка\" + -0.263*\"швейный\" + 0.261*\"ягуар\" + -0.241*\"машинка\" + -0.228*\"лапка\" + -0.209*\"машин\" + -0.182*\"строчка\" + -0.170*\"шпулька\" + -0.138*\"jaguar\" + -0.135*\"верхний\"\n",
      "***\n",
      "-0.390*\"фильм\" + 0.289*\"ягуар\" + -0.273*\"вебер\" + -0.241*\"это\" + -0.194*\"свой\" + -0.175*\"герой\" + -0.168*\"который\" + -0.139*\"перрен\" + -0.121*\"ришар\" + -0.119*\"дуэт\"\n",
      "***\n",
      "-0.322*\"jaguar\" + -0.321*\"год\" + 0.237*\"нитка\" + -0.213*\"модель\" + 0.188*\"фильм\" + -0.169*\"компания\" + -0.158*\"напиток\" + -0.145*\"автомобиль\" + 0.138*\"шпулька\" + 0.131*\"вебер\"\n",
      "***\n",
      "-0.701*\"напиток\" + -0.119*\"алкоголь\" + -0.119*\"банка\" + -0.099*\"энергетический\" + 0.099*\"машинка\" + -0.095*\"е\" + -0.092*\"вред\" + -0.090*\"человек\" + -0.087*\"нитка\" + -0.082*\"слабоалкогольный\"\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "lsi = models.lsimodel.LsiModel(corpus=corpus, id2word=dictionary, num_topics = 10)\n",
    "for topic in lsi.print_topics(5):\n",
    "    print topic\n",
    "    print '***'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014*ягуар + 0.007*jaguar + 0.007*год + 0.006*швейный + 0.005*который + 0.005*машинка + 0.004*машин + 0.004*свой + 0.004*модель + 0.004*строчка\n",
      "***\n",
      "0.024*ягуар + 0.008*год + 0.005*свой + 0.005*это + 0.004*нитка + 0.004*фильм + 0.004*животный + 0.004*человек + 0.004*который + 0.003*самка\n",
      "***\n",
      "0.014*ягуар + 0.009*год + 0.007*который + 0.006*jaguar + 0.005*модель + 0.005*фильм + 0.005*иметь + 0.004*напиток + 0.004*это + 0.004*автомобиль\n",
      "***\n",
      "0.015*ягуар + 0.012*напиток + 0.006*год + 0.005*это + 0.004*свой + 0.004*который + 0.004*jaguar + 0.003*один + 0.003*стать + 0.003*человек\n",
      "***\n",
      "0.016*ягуар + 0.005*напиток + 0.004*год + 0.004*человек + 0.004*пятно + 0.004*чёрный + 0.003*jaguar + 0.003*также + 0.002*вод + 0.002*это\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics = 15)\n",
    "for topic in lda.print_topics(5):\n",
    "    print topic\n",
    "    print '***'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (SageMath)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "Russian_tf-idf_VSM.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
